import React, { Component } from 'react';
import { Link } from 'react-router-dom';
import '../css/pages/methodologypage.css';


export default class MethodologyPage extends Component {

    render() {
        return (
            <div className="methodology-page__container">
                <div className="inner-content">
                    <h4>Methodology</h4>
                    <img style={{"object-fit": "scale-down", "width": "100%"}} src="https://imgur.com/68bbVJ7.jpg"></img>
                    <p>
                        Our system has three main techniques to extract the desired features from the images. They are Local Binary Patterns (LBP), Shapes, and Optical Character Recognition (OCR). The implementation details for each technique will be explained in later sections. The output for each of those technique are combined together to form a single feature vector. The procedure for doing so is also known as Early Fusion, which is one of the Fusion techniques as per our requirement for the project. 
                    </p>
                    <p>
                        The feature vectors obtained from each image in our dataset will be used to train and test machine learning classification models. Further details will be explained later on in this report. The machine learning classification models we would be using are Support Vector Machines (SVM) and K Nearest Neighbors (KNN). We have included two variants of SVM in our experiment, namely a linear SVM and a RBF kernel (non-linear) SVM. The difference in between a linear SVM and a non-linear SVM lies in their decision boundaries to classify data points. For simplicity, a linear SVM decision boundary can be represented as a line in 2D space, or a plane in 3D, or a hyperplane for higher dimensional spaces, while the decision boundary for a non-linear SVM is represented as a curve in some dimensional space. In the case of binary classification problems, like the problem we are dealing with, data points that reside on one side of the decision boundary belong to one class whereas data points on the other side belong to another class. 
                    </p>
                    <p>
                        KNN, on the other hand, is a much simpler algorithm but is effective in solving some problems. For a new given data point, the prediction class for the new data point is determined based on the majority class of its nearest N neighboring data points, where N > 0. The nearest neighbors are determined based on some distance measure, one such example would be the Euclidean Distance. For example, for a 3 Nearest Neighbor model, if a new data point has 2 neighbors which belong to Class A and 1 neighbor which belong to Class B, then the new data point would be classified under Class A, as its the majority class among the 3 neighbors.
                    </p>
                    <p>
                        The Python module we are using for the classification models is scikit-learn, as it has convenience classes for both SVM and KNN, and also for its API simplicity. It also useful in the sense that it has a number of APIs for measuring classification performance, consisting of widely-used metrics for classification. All of our experimental results are generated by scikit-learn, hence no manual work was done by us to achieve the same thing.
                    </p>
                    <h4>Local Binary Patterns</h4>
                    <img style={{"object-fit": "scale-down", "width": "100%"}} src="https://imgur.com/jmcyAHx.jpg"></img>
                    <p>
                        Local binary patterns makes use of the textural information in an image for classification. Ever since it was first described in 1994 [7] by Ojala et al., it has been widely considered to be a robust feature for texture classification, and is also computationally extremely simple. LBP measures generally can perform very well, no matter the form of the texture as it is gray-scale invariant and can be modified with a simple contrast measure to make it even more powerful. These few characteristics of LBP makes it a highly suitable descriptor to be applied in the analysis of biomedical images.
                    </p>
                    <p>
                        Extensive research has been done on LBP during the previous semester. One useful approach to the original operator that we have found is the ‘uniform pattern’ approach. This approach reduces the length of the histogram vector and also makes it invariant to rotations. Taking into account of the available time and resources that we have, we will only be considering the implementation of the ‘uniform pattern’ approach variant as it is both simple and robust. We cannot assume that the subfigures in the compound images are perfectly oriented in a 90 degree angle. Hence, the fact that uniform LBP is rotational invariant helps us overcome this limitation. In a general rotation invariant LBP approach, the LBP operator that characterizes the spatial structure of the local image texture is based on a circularly symmetric neighbour set of P points on a circle of R radius.
                    </p>
                    <p>
                        After specifying the number of points and the radius, our implementation of LBP computes the LBP representation of the image with the help of scikit-image’s feature.local_binary_pattern() method. The histogram is then attained by rearranging the bins according to the number of points with numpy’s arrange() method. The histogram then undergoes normalization before it is returned to the parent method.
                    </p>
                    <p>
                        Further recalling previous research done on classification of salient regions using LBP, we are then inspired by region competition, which is a hybrid statistical segmentation method involving segmenting the image into homogenous subregions. In region competition, the image is segmented into regions of similar size, and then several randomly selected seeds are assigned to random regions. These regions are then iteratively merged and grown using a greedy algorithm until the regions in the image converges [8]. It is to be noted that this method does not solve the problem of compound image detection as it merely segments the image into regions with different textural classifications. Although the output of region competition is seen to be a qualifiable input for training our classifiers, given the time constraint that we had and the difficulty of implementing the region competition algorithm ourselves, we have opted for a simpler approach which also involves subdividing the image into homogenous regions.
                    </p>
                    <p>
                        We start by resizing the image into a standardized 600 by 600 pixels size as to fix the length of the resulting feature vector. Next, the image is subdivided into m2​ ​ regions, where m is the width and height of each subregion. These subregions are then converted to grayscale, and is described with our implementation of uniform LBP with 8 points on a circle of a radius of 24 pixels. We then concatenate the resulting histograms sequentially with each other, creating a one-dimension feature vector. This feature vector will be the training input to the classifiers, and has proven to be good enough to yield moderate accurateness within the results.
                    </p>
                    <h4>Shapes Features</h4>
                    <img style={{"object-fit": "scale-down", "width": "100%"}} src="https://imgur.com/eI0xftd.jpg"></img>
                    <p>
                        The second method we have implemented for detecting biomedical compound figures is by using the shape features of the image. Every non-blank images bound to have a shape that is easily recognisable with our eyes. For a computer, images are visualize in an array of pixel values which is completely different to how humans perceive an image. Relating to our project, several biomedical compound figures are distinguishable by the gaps between the different sub figures where it creates a square or rectangular like box at the border of it. These gaps are commonly used to separate the figures from the rest of the images that are present in the same image as shown in ​Figure 5. ​To detect these boxes using the computer, we have implemented a box detection algorithm similarly to how (You, D et.al, n.d.) did with finding contours of an object using the 4 chain-code representation in [11]. Instead of doing it in a 8 directional, 4 code manner, we detect only the vertical and horizontal lines in the image, since boxes are usually right angled lines made up of vertical and horizontal lines. The next section shows the algorithm for detecting shape features of a compound figure.
                    </p>
                    <p>
                        The algorithm takes in an input image and processed it by applying a grayscale filter to reduce the amount of colour details. A median blur filter was chosen to reduce noise present in the grayscale image as the median blur was shown to preserve useful details that are needed later on in extracting our features. The blurred image is then converted into binarized image and the resulting image was inverted. The image is inverted for a contour searching algorithm that is in openCV, which is the findContours function.
                    </p>
                    <p>
                        The pre-processed image is further processed by extracting the vertical and horizontal lines from the inverted binarized image. To extract the respective lines of the image, a kernel is specified for the structuring element to detect the horizontal and vertical lines. The structuring element is an array in which for the horizontal line is a single rows of 1’s and single column of 1’s for vertical line. The structuring element are then used to erode and dilate the pre processed image based on the specified kernel. This morphological operation was reference from the openCV documentation for morphological operation. The resulting images can be seen in ​Figure 7. ​The reason why we use this technique is because edge detection like Canny and Sobel gives too much unnecessary edges that are not needed in our box detection algorithm.
                    </p>
                    <p>
                        Both of these vertical lines and horizontal lines images are then merged together to produce a vertical and horizontal image as shown in ​Figure 7​. This combined image is then used for obtaining our box by using the function findContours. FindContours is a built-in function from openCV which detects contours of a white object from a black background. The detected contours are visualized by getting it’s bounding rectangle.
                    </p>
                    <h4>Optical Character Recognition</h4>
                    <img style={{"object-fit": "scale-down", "width": "100%"}} src="https://imgur.com/7f3wxSa.jpg"></img>
                    <p>
                        OCR can be useful in identifying certain features of an image which may be unique to compound figures. The kind of features in this case involves texts that are embedded in the compound images. There are more textual information in compound images that contain graphs, as can be seen in ​Figure 9​. For such images, there is a high chance that certain texts would be repeated in such cases for compound images. Therefore, the main goal of OCR in our system is to be able to identify and extract text regions in the image, use an OCR engine to recognize the characters in those text regions, and then put the recognized texts into a dictionary. The dictionary would hold the number of occurrences for a particular text in an image.
                    </p>
                    <p>
                        Standard image preprocessing techniques are applied to the image before any further processing. The preprocessing techniques include converting the image to grayscale, removing noises from the image using a Gaussian blur, sharpening the edges in the image using an unsharp technique, and finally applying Canny edge detection to get an edge image. The edge image is crucial for the text detection algorithm which would be explained later on. Since the edges play an important part in this pipeline, it makes sense to improve the edge detection procedure by blurring the image and apply some sharpening.
                    </p>
                    <p>
                        The text detection algorithm follows the implementation of Xu and Krauthammer [10]. It uses an iterative and divide and conquer approach to approximate the text regions in an image. The algorithm makes the assumption that text regions in an image normally has higher density of edge pixels. Therefore, the algorithm attempts to detect regions in the image that meets a threshold on the number of edge pixels. This is why the edge detection procedure in the preprocessing stage was crucial.
                    </p>
                    <p>
                        To determine the density of edge pixels in a particular region, a horizontal histogram is first obtained for the region. Values in a horizontal histogram are the counts or the occurences of edge pixels in each column of the region. At the start of the iteration, the region would be the entire image itself. The obtained vertical histogram is divided into several segments, the segmentation is done by thresholding the values. Values above the threshold become 1 whereas values below the threshold become 0. Thus, areas where there is a high density of edge pixels should form long segments of 1s.
                    </p>
                    <p>
                        Once the histogram values have been segmented, we filter out the segments that have length less than 3, as these segments probably correspond to non-text regions. Those segments now yields subregions. We then derive a vertical histogram for each of those subregions. This time the vertical histogram corresponds to the count of edge pixels in each row of the subregions. The procedure for the vertical histogram is similar to the horizontal histogram earlier, hence the histogram is again segmented by thresholding, and the considerably short segments are filtered out.
                    </p>
                    <p>
                        The pairs of segments obtained from the horizontal histogram and the vertical histogram corresponds to a rectangular region which is possibly a text region in the image. Some post processing is done such that the bounding box of the region nicely fits with the text. We used a minimum coverage algorithm to do just that. It is simply a two stage process, where we first attempt to minimally expand the bounding box until there are no longer any edge pixels nearby, and then proceeded to maximally shrinking the bounding box without excluding the edge pixels that are already within it.
                    </p>
                    <p>
                        We have decided to use Tesseract as the OCR engine of our choice, which is open-sourced and has its development sponsored by Google. Its usage is very simple, all it needs is a nicely preprocessed image to work with, in order to produce quality recognition results. The images that would be given to Tesseract would be the text regions that was obtained from the previous stage.
                    </p>
                    <p>
                        After feeding each text region at a time to the Tesseract engine, we can now iterate through the dictionary and determine what are the words that occured beyond a threshold that we set, in our case we only consider the words that occur more than once. Recall that we are only interested in the words that occured N number of times in the image, as it could potentially correspond to a compound image. We accumulate the occurences of words that occurred more than once as a percentage of the the total word occurrences in the dictionary.
                    </p>
                    <h4>Outcomes</h4>
                    <img style={{"object-fit": "scale-down", "width": "100%"}} src="https://imgur.com/rOhm8pp.jpg"></img>
                    <img style={{"object-fit": "scale-down", "width": "100%"}} src="https://imgur.com/RH5UHhc.jpg"></img>
                    <p>
                        From the results shown, it is clear that RBF Kernel SVM has the best classification performance among the three models. In terms of test set accuracy, RBF Kernel SVM performed the best, with a score of 0.8, followed by KNN with a score of 0.77, and Linear SVM with a score of 0.75. Comparing in terms of accuracy alone is insufficient and possibly unreliable. In a general case, the accuracy can be misleading or biased, especially on imbalanced datasets, where the model could have a tendency to predict one class more often than the other. This is where other classification metrics such as Precision, Recall and F1 score come in. Before going any further, some intuition will be given about using these metrics.
                    </p>
                    <p>
                        In any binary classification problem, there is a positive class and a negative class. In our case, for the metrics concerning compound figures, the positive class is the class of compound figures and the negative class is the class of non-compound figures, whereas for the non-compound metrics, it is the opposite. Precision is a measurement that determines the correctness of the model in classifying samples as the positive class, and it does so as a percentage of the total samples that was classified correctly as the positive class (TP) and samples that was incorrectly classified as a positive class (FP). Recall, on the other hand, measures how many of the positive class samples are correctly predicted as the positive class by the model, as a percentage of the total samples that are correctly classified as the positive class (TP) and the samples which are incorrectly classified as the negative class (FN), hence it only looks at all the actual positive class samples. The F1 score metric is merely there for convenience when comparing which model is the best, as it is a combination of both Precision and Recall. Sometimes it is harder to say which model is the best if one has higher Precision or Recall or vice versa.
                    </p>
                    <p>
                        Going back to the classification results tables, RBF Kernel SVM is still a clear winner in terms of the Precision metric. Its Precision values for both compound and non-compound figures are 0.81 and 0.79 respectively. KNN again takes the second spot in terms of Precision, where its values are 0.8 and 0.74, and finally Linear SVM has values of 0.76 and 0.73. For the case of Recall, RBF Kernel SVM and KNN share the same value of 0.81 for non-compound figures, however RBF Kernel SVM has significantly higher value of 0.79 while KNN has 0.73 for compound figures. Linear SVM is significantly lower than both models for non-compound, but it is on par with KNN for compound figures. This means that, in terms of classifying the compound figures correctly as compound figures, Linear SVM and KNN are equal in that regard, while RBF Kernel outperforms both. What separates KNN from Linear SVM, however, is that it classifies more non-compound figures correctly, as can be seen in their Recall values for non-compound, but is equal to RBF Kernel SVM in that respect.
                    </p>
                </div>
                <div className="button-row">
                    <Link to="/fyp-acid/">
                        <button className="btn btn-success">Back</button>
                    </Link>
                </div>
            </div>
        );
    };
};
